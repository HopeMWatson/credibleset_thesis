---
title: "Draft thesis 20_07_2018"
output:
  pdf_document: default
  word_document: default
bibliography: library.bib
---

---
title: "Fine mapping genetic associations"
output: pdf_document
   
---
\begin{center}

\LARGE _Author:_ \newline
\Large Hope Watson 

\LARGE _Supervisors:_ \newline
\Large Dr. Chris Wallace 
\Large Dr. Jennifer Asimit 
\Large Dr. Deborah Thompson 


\large August 2018

\large Thesis submitted in partial fulfillment of the degree of Master of Philosophy in Epidemiology

\end{center}


\pagebreak

#Declaration 
I declare that this thesis is the result of my own work. All sources are fully acknowledged and
appropriately cited. The contents of this submission have not been used to any extent for a comparable
purpose, i.e., in partial fulfilment of another degree or diploma at any university or institute of
higher learning.

__Word Count (excluding tables, figures, and references):__

#Table of Contents 

\tableofcontents



    How is a credible set different from a credible interval? Is a credible set a collection of credible intervals? A credible set is discrete with set points (where SNPs occur) whereas credible set has a continuous distribution.

    Why does the lead SNP cause signal noise? The lead SNP which has the largest p value is almost never the causal variant. By adjusting for the SNPS that are in high LD with the lead SNP

    Go over plots:

knitr::include_graphics('./conditional association analysis conditioning on the lead SNP.jpg')

    Why is the Bayesian method not influenced by odds ratios and RAF/MAF differences or the power of the study (related to n)? Doesn't n always influence how much we understand LD? Bayesian methods incorporate the variance into the approximate bayes factor and therefore, since it is incorporated, power, variance, and any properties related to sample size are already adjusted/accounted for.



#Basic information ##Credible set and credible interval

Credible set, is a set of objects, where it is believed one of the objects is the causal variant. Each object is a SNP, that has an assigned posterior probability of being the causal variant.

The size of the credible set is the probability the credible set contains the causal variant.

Credible set - confidence that the causal variant is included A credible set has fixed boundaries and unknown parameter (mean, proportion, rate, etc.) Contrast to frequentist where parameter is fixed and boundaries are not. Very importantly in frequentist statisitcs it only uses the data to derive these measures, whereas bayesian utilises prior distribution. In genetics this is important, so we are able to annotate information about genetic variation, such as if it is in a coding or non-coding region.

Confidence - sum of the posterior probabilities

## What is fine-mapping A process to refine these lists of associated variants to a credible set most likely to include the causal variant. Assign well-calibrated probabiliites of causalility to candidate variants. Then connect these variants to likely genes - functional annotation.

#Notes from Reading ##Strategies for fine-mapping complex traits ###Intro eQTL - expression quantitative trait loci - level of gene expression in a particular tissue ###Principles of fine-mapping 1)P-values are influenced by study specific factors - power (related to n) and MAF and OR 2)LD with the lead SNP - still ignores study properties, because higher power can differentiate SNPs in higher LD. 3)Bayesian Framework ###Bayesian Framework Bayes Factor (BF) - ratio of the likelihood probability of two competing hypotheses Each variant is measured using a bayes factor and used to give posterior probability. Posterior Probability (PP) ratio of evidence for each variant versus all others.

## Evaluating the Performance of Fine-Mapping Strategies at Common Variant GWAS Loci

# Data Source ##1000 Genomes Project Provides reference panel data. This is the gold standard data that still has coverage issues.

# Motivation 
Genetic testing is increasing at rapid rates, as the cost of genotyping continues to become much cheaper. Genome Wide Assocation Studies began around 15 years ago. Many strong genetic assocations with simple heritability mechanisms have already been identified, while genetic assocations with complex genetic mechanisms and disease aetiology continue to be challenging. Determining which assocations, that is which mutations to investigate is both time and cost intensive. The overall aim of this study is to reduce the number of candidate causual SNPs to be considered out of a credible set of SNPs. This is done by improving the current methods of how the credible set is derived. 

# Introduction
The introduction is split into two sections of background to provide clarity from 1) scientific side - genetics, and 2) mathematics side - Bayesian statistics, described in light of the genetics information. 

# Genetics 

## What are genetic associations?
A genetic association is a change in the genetic code that alters the risk of disease. Changes occur in the form of single nucleotide polymorphisms (SNPs), copy number variants (CNV), and indels (micro and microsatellites) 
Genetic association studies -> find associations.

## What is GWAS? 
Genome wide association studies identify genetic positions associated with different disease risks in a population

## What is LD?
Linkage disequilibrium is the correlation structure that exists among DNA variants in the current human genome as a result of historical evolutionary forces, particularly finite population size (genetic drift), mutation, recombination rate, and natural selection [@Visscher2017]. In short, this correlation structure means that SNPs are not statistically independent of each other. LD makes it possible for 80 million SNPs to be arrayed by only 500kb-1Mb SNPs. However, LD also makes it difficult to indicate which SNP is causal and which is a highly correlated neighboring SNP. 
Different genes have different recombination probabilities, and therefore genetic distance does not equal physical distance. 

LD is expressed by 


## Fine Mapping 
Fine mapping is a process in which researchers try and identify the most likely single or set of variants within which the causal variant should lie. 

## Types of Methods 

### lead SNP (smallest p)
A previous and simple approach was to consider all SNPs of a certain threshold (5 x 10^-8) as potential candidates for causality. There are several issues with this, as p-values are influenced by study specific factors such as power (sample size), minor allele frequency, and the effect size (rarely known). Differences in these variables make genetic studies difficult to compare. [@Spain2015]

### lead SNP + LD friends (eg r2>0.8)
Another method used was to take the lead SNP, that is the SNP with the highest P-value, and compare it with a certain LD threshold, typically r^2>0.8 and consider it as a potentially causal SNP. This method still ignores properties of the study or locus, as greater power can differentiate SNPs in higher LD 

### Bayesian method

Association p values converted to Bayesian posterior probabilities of causality under specific assumptions.  These can be summed over sets of variants to generate a posterior probability a causal variant lies in any given set.

Bayesian methods are not directly influenced by study 

Credible sets - involves sorting

Aims of thesis
- assess Bayesian fine mapping
- hopefully improve it!



## Posterior Probability 
The Bayesian's belief in a binary hypothesis (eg this SNP is causal vs this SNP is not causal) after seeing the data.  Note difference to prior belief.  Bayes Theoreom

\begin{equation}
\label{eq-Bayes Theorem}
P(X=x|D)
= \frac{P(X=x) P(D|X=x)}{P(D)}
\end{equation}

\begin{equation}
\label{eq-Bayes Theorem}
{P(X=x|D)}
 = \frac{P(D|X=x) P(X=x)}{\sum(P(D|X=y)(P(X=y))}
\end{equation}

\begin{equation}
\label{eq-Bayes Theorem}
{P(X=x|D)}
 \propto {P(D|X=x) P(X=x)}
\end{equation}

statistical probability that a hypothesis is true, calculated in light of relevant observations. Relavant observations may be defined as both the prior information and the new data that is being analysed, both which together generate the posterior probability. For this reason, the posterior probability is proportionally related to the prior, by the expression of the newly observed data. 

# Bayesian Statistics 
## Contrast with Frequentist 
In a frequentist framework a parameter of interest (mean, proportion, rate)  is fixed and only varies due to sampling variation. The process of inferring the value of the parameter works by considering every possible result that a study could potentially generate. That is under the same conditions, what would be observed under multiple parallel imaginary samples. However this is not possible many biological phenomia, where a hypothesis such as 'What is the porobabilty that a SNP is causal, given the data currently accrued?' #Kirkwood2005 There is no interpretation of this hypothesis in frequentist terms, since there is a fixed but unknown quantity of the SNP (functional genomic issues such as gene expression under the frequentist interpretation).  In frequentist statistics the null hypothesis is assumed to be true; that is, it is the starting point of all inference that the observed data is compared to. The P-value reflects the probability of observing data as or more extreme than the data actually observed in current study, given the null hpyothesis; this framework does not allow for competing hypotheses. [@Gurrin2000]. A 95% confidence interval is interpreted as if new data were to be repeatedly sampled and analysed, 19 out of 20 intervals would include the true quantity parameter being estimated. 
Bayesian statistics incorporate pre-exisitng information into analyses, in the form of a prior, which is a belief of the distribution of the data before any new data has been observed or incorporated in analysis. This can come a variety of forms such as previous studies, consultation with experts, or theorectical biological models. Under these conditions, spelt out by the Bayes Theorem, a parameter is correctly interpreted as the probability of a hypothesis , given the observed data. 


## Credible interval
A credible interval is a range of values within which an unobserved parameter values falls within a particular subjective probability.  [@Edwards1963]. Unlike in frequentist statistics, in Bayesian we are asking what is the likelihood of a hypothesis, given the data we have observed. The data we have observed is represented by both priors - information that was already known that can be incorporated into analysis of the newly observed data. 

## Credible set
A credible set is a set of objects, where it is believed one of the objects is the causal variant. Each object is a SNP, that has an assigned posterior probability of being the causal variant. The set is created by setting a threshold, where 


## Issues with Credible Sets 
There is currently no explicit quantitiative definition for a credible set, 

## Size of credible set
For one credible set, the size of the credible set is the belief that the credible set contains the causal variant (eg 90%). 

## Coverage of credible set
If we repeat analysing a list of credible sets, coverage is the amount of times the causal variant is in that credible set. Here coverage can be defined as number of times the causal variant was in the credible set out of the number of simulations run. This can be expressed as a percentage where if 10 credible sets are analysed and 9 of these credible sets include the causal variant, there is 90% coverage. 


# Link with Frequentist 
Link between a frequentist confidence interval and a bayesian credible interval with an uninformative prior. When Bayesian framework is used, but the prior belief is uninformative, the posterior probability is derived from the observed data, defined by the likelihood. An uninformative prior mathematically takes the form of uniform distribution, so that any probability in the distribution has the same proportion. This is similar to frequentist probabilities, which only use the data to estimate the unknown parameter. Under this framework, a parameter of interest credible interval, the posterior probability a SNP is causal can be interpreted 



## Scope of the Study: 
This study is done under the one causal variant assumption, meaning there is only one causal variant per credible set. This assumption creates some limitations that are outlined here. 
Colocalization is when a single SNP is associated with multiple phenotypes. Some methods used for colocalization calculate posterior probabilities works by fine mapping each trait under a single causal variant assumption. The two posterior probabilites are then integrated over to calculate probabilities that those variants are shared [@Wallace2016]. Therefore, this analysis may be able to be extended to involve colocalization in the future but is not addressed at this time.  

Copy number variants (CNVs) and variable number tandem repeat (VNTRs) consisting of microsatellites and minisatellites mutations, may account for a 10-15% of heritable gene expression variation in humans [@Gymrek2015]. Repetitive DNA is not easily analysed by next generation DNA sequencing methods, which struggle with homopolymeric tracts, that is, parts of the genotype that have the same base pair type repeated many times. Bi-allelic SNPs are not subject to this issue in the way CNVs and VNTRs are, which means they are more easily assayed. This analysis does not address the potential importance of CNVs and VNTRs and assumes that the causal variant can is a bi-allelic SNP. 

Functional annotations are added information known about a SNP in how it effects expressed phenotypes. Functional annotations are typically expressed as coding or non-coding regions of the genes 
Although not explored here, methods outlined in this study could be explored by altering the assigned prior value of a SNP in finemap.abf. Reweighting of the posterior probabilites can be done by using fGWAS [@Pickrell2014] or PAINTOR methods [@Kichaev2014]

Different trans-ancestries have different LD patterns. This study does not address any trans-ancestry differences or geographical analysis of subpopulations and their respective potential subtypes.  A SNP that has a small p-value across groups that have different genetic architecture shows a stronger genetic assocation to a phenotype than if it were present in only one of the groups. In this analysis only one LD matrix is considered at a time, but this could be extended in the future, particularly with relevant open data access from sources such as 1000 Genomes Project. [find spot to cite Jenna's papers here ]. 




## Aims 
This study aims to provide a history of fine mapping and its methods 

# Current Practices in Fine Mapping 

## Entropy
A measure of a system's disorder. 
-sum p log(p)

# Methods

## Why use Simulations?
Simulations are the gold standard in evaluating methodologies relevant to causal variants. In the "real-world", that is genotyping for actual phenotypes of interest (typically a disease), we do not know the causal variant. It is not possible evaluate both real-world data and the robustness of the methodology simulataneously. This is because the lack of certainty and variability of both parameters may be the reason for the outcomes; a basic concept of the scientific method -- test one variable at a time and hold all other variables the same as controls. 

## Running Simulations

Simulations were run by through a series of steps, leveraging relevant available R packages. The simGWAS package was utilised to created simulated 
## Coverage Evauluation 

how you fixed (if we do!)

# Results

plots of coverage vs size for unordered 'credible sets'

plots of coverage vs size

plots of entropy vs coverage-size

fix!

# Discussion

# References

\bibliographystyle{unsrt} 
