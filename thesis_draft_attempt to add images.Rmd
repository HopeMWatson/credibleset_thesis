---
title: "thesis_draft_attempt to add images"
output: pdf_document
---


\usepackage{graphicx}
\graphicspath{ {C:\Users\Hope\Documents\Thesis\thesisimages} }


---
title: "Fine mapping genetic association"
output:
  pdf_document: default
  word_document: default
bibliography: library.bib
---


\begin{center}

\LARGE \textbf{Author:}
\Large Hope Watson 

\LARGE \textbf{Supervisors:}

\Large Dr. Chris Wallace 
\Large Dr. Jennifer Asimit 
\Large Dr. Deborah Thompson 


\large August 2018

\large Thesis submitted in partial fulfillment of the degree of Master of Philosophy in Epidemiology

\end{center}


\pagebreak

# Declaration 
I declare that this thesis is the result of my own work. All sources are fully acknowledged and
appropriately cited. The contents of this submission have not been used to any extent for a comparable
purpose, i.e., in partial fulfilment of another degree or diploma at any university or institute of
higher learning.


\pagebreak
# Acknowledgements 
I would like to thank the many people who made this thesis possible. Without their help I would not have grown as an academic and begin to change how I see phenomia occuring in the world. I especially like to thank Dr. Chris Wallace and Dr. Jennifer Asimit, for continuously helping me all throughout this thesis and assisting me in the many questions I asked. I would like to thank Dr. Deborah Thompson for helping edit this thesis and being a devoted supervisor and mentor during my entire year, both academically and personally. 



__Word Count (excluding tables, figures, and references):__

\pagebreak

# Table of Contents 

\tableofcontents


\listoffigures



# Introduction 
Genetic testing is increasing at rapid rates, as the cost of genotyping continues to become much cheaper. Genome Wide Assocation Studies began around 15 years ago. Many strong genetic assocations with simple heritability mechanisms have already been identified, while genetic assocations with complex genetic mechanisms and disease aetiology continue to be challenging. Determining which assocations, that is which mutations to investigate is both time and cost intensive. The overall aim of this study is to reduce the number of candidate causual SNPs to be considered out of a credible set of SNPs. This is done by improving the current methods of how the credible set is derived. This process enables statistical assocations to be converted into target genes. 

# Background
The introduction is split into two sections of background to provide clarity from 1) scientific side - genetics, and 2) mathematics side - Bayesian statistics, described in light of the genetics information. 

# Genetics 

## What are genetic associations?
A genetic association is a change in the genetic code that alters the risk of disease. Changes occur in the form of single nucleotide polymorphisms (SNPs), copy number variants (CNV), and indels (micro and microsatellites) 
Genetic association studies -> find associations.

## What is GWAS? 
Genome wide association studies identify genetic positions (loci) associated with different disease risks in a population. 

## Important Terms

A phenotype is an expressed trait of interest. This is typically a disease such as cancer or heart disease. In a case-control study, a person with the expressed phenotype of interest is a case, and the person without the expressed phenotype is a control. 

A genotype is the genetic makeup of a organism that will determine its phenotype. The genetic make up is a sequence of base pairs.

A single nucleotide polymorphism (SNP) is a mutation where one base pair is different than the reference genome. Most SNPs are bi-allelic; this makes them easy to be analysed through a binary statistical inferences. 

A gene a stretch of DNA that codes for a particular protein. A haplotype is a group of genes within an organism that was inherited together from a single parent. Haplotypes also refer to the inheritence of clusters of single nucleotide polymorphisms (SNPs), which are variations at single positions in the DNA sequence among individuals [https://www.nature.com/scitable/definition/haplotype-haplotypes-142]. Haplotypes are necessary in creating linkage disequilibrium matrices (discussed below). 


An allele is alternataive forms of a gene/trait that are found at the same place on a chromosome. Minor Allele Frequency (MAF) is the frequency (count) at which the second most common allele occurs in a given population. The MAF gives a reference to how commmon or rare a variant is. A MAF of 0.5 means that the frequency of that specific allele is present in 50% of the population.

### Understanding GWAS Manhattan Plot

\includegraphics{Manhattan_Plot}

Each dot on a Manhattan plot represents one haplotype. The x axis is the loci, which is the position of the SNP on the chromosome. On the y axis is the -log p value of the association of the SNP in the cases compared to the control group. Dots farther up on the y axis represent a SNP that has a stronger association with the phenotype. The null hypothesis is that each SNP has no assocation with the phenotype. Through GWAS, candidate genes can be identified. In plots where there are multiple highly associated SNPs, this is an indication of pleitropy, which means that the same gene effects more than one phenotype. 

## What is LD?
Linkage disequilibrium is the correlation structure that exists among DNA variants in the current human genome as a result of historical evolutionary forces, particularly finite population size (genetic drift), mutation, recombination rate, and natural selection [@Visscher2017]. In short, this correlation structure means that SNPs are not statistically independent of each other. LD makes it possible for 80 million SNPs to be arrayed by only 500kb-1Mb SNPs. However, LD also makes it difficult to indicate which SNP is causal and which is a highly correlated neighboring SNP. 
Different genes have different recombination probabilities, and therefore genetic distance does not equal physical distance. 

LD is expressed by a matrix, where 

## Fine Mapping 
Fine mapping is a process in which researchers try and identify the most likely single or set of variants within which the causal variant should be present. The process is done by assigning well-calibrate probabilites of causality to each candidate variant [@Spain2015]. 


## Types of Methods in Fine Mapping

### Lead SNP (smallest p-value)
A previous and simple approach was to consider all SNPs of a certain threshold (5 x 10^-8) as potential candidates for causality. There are several issues with this, as p-values are influenced by study specific factors such as power (sample size), minor allele frequency, and the effect size (rarely known). Differences in these variables make genetic studies difficult to compare. [@Spain2015]

### Lead SNP and LD "friends (i.e. r\textsuperscript{2}>0.8)

Another method used was to take the lead SNP, that is the SNP with the highest p-value, and compare it with a certain LD threshold, typically r\textsuperscript{2}>0.8) and consider these neighboring SNPs as the potential causal SNP. This method still ignores properties of the study or locus, as greater power can differentiate SNPs in higher LD. 

### Bayesian method

Association p-values converted to Bayesian posterior probabilities of causality under specific assumptions [@Stephens2009] [@Wakefield2007].  These can be summed over sets of variants to generate a posterior probability a causal variant lies in any given set. The posterior probabilities (pp) are the ratio of evidence for each variant beign causal versus all the others. This is expressed as 
\begin{equation}
\frac{pp \sum
\end{equation}

Bayesian methods are not directly influenced by study 

Credible sets - involves sorting

Aims of thesis
- assess Bayesian fine mapping
- hopefully improve it!



## Posterior Probability 
The Bayesian's belief in a binary hypothesis (eg this SNP is causal vs this SNP is not causal) after seeing the data.  Note difference to prior belief.  Bayes Theoreom

\begin{equation}
\label{eq-Bayes Theorem}
P(X=x|D)
= \frac{P(X=x) P(D|X=x)}{P(D)}
\end{equation}

\begin{equation}
\label{eq-Bayes Theorem}
{P(X=x|D)}
 = \frac{P(D|X=x) P(X=x)}{\sum(P(D|X=y)(P(X=y))}
\end{equation}

\begin{equation}
\label{eq-Bayes Theorem}
{P(X=x|D)}
 \propto {P(D|X=x) P(X=x)}
\end{equation}

statistical probability that a hypothesis is true, calculated in light of relevant observations. Relavant observations may be defined as both the prior information and the new data that is being analysed, both which together generate the posterior probability. For this reason, the posterior probability is proportionally related to the prior, by the expression of the newly observed data. 

#Relationship between p-values and Posterior Probabilities 
The relationship between p-values and posterior probabilities is an inverse relationship. The smaller the p-value which represents the likelihood of observing the data given the null hypothesis, the higher the posterior probability. 

- typically, the SNP is not associated with the 

# Bayesian Statistics 
## Contrast with Frequentist 
In a frequentist framework a parameter of interest (mean, proportion, rate)  is fixed and only varies due to sampling variation. The process of inferring the value of the parameter works by considering every possible result that a study could potentially generate. That is under the same conditions, what would be observed under multiple parallel imaginary samples. However this is not possible many biological phenomia, where a hypothesis such as 'What is the porobabilty that a SNP is causal, given the data currently accrued?' #Kirkwood2005 There is no interpretation of this hypothesis in frequentist terms, since there is a fixed but unknown quantity of the SNP (functional genomic issues such as gene expression under the frequentist interpretation).  In frequentist statistics the null hypothesis is assumed to be true; that is, it is the starting point of all inference that the observed data is compared to. The P-value reflects the probability of observing data as or more extreme than the data actually observed in current study, given the null hpyothesis; this framework does not allow for competing hypotheses. [@Gurrin2000]. A 95% confidence interval is interpreted as if new data were to be repeatedly sampled and analysed, 19 out of 20 intervals would include the true quantity parameter being estimated. 
Bayesian statistics incorporate pre-exisitng information into analyses, in the form of a prior, which is a belief of the distribution of the data before any new data has been observed or incorporated in analysis. This can come a variety of forms such as previous studies, consultation with experts, or theorectical biological models. Under these conditions, spelt out by the Bayes Theorem, a parameter is correctly interpreted as the probability of a hypothesis , given the observed data. 


## Credible interval
A credible interval is a range of values within which an unobserved parameter values falls within a particular subjective probability [Edwards1963]. 


--Unlike in frequentist statistics, in Bayesian we are asking what is the likelihood of a hypothesis, given the data we have observed. The data we have observed is represented by both priors - information that was already known that can be incorporated into analysis of the newly observed data. 

## Credible set

A set of SNPs that contains the causal variant with a pre-specified probability. Each SNP has an assigned prior probability and a posterior probability is calculated, using the likelihood of the data observed [Cite Jenna somewhere here]. The set is determined by a threshold of desired cumulative poster probabiities to reach this cutoff. 

## Issues with Credible Sets 
There is currently no explicit quantitiative definition for a credible set as there is for a credible interval. A credible assumes a continuous distribution, while a credible set is made up of a discrete distribution for the SNP 

## Size of credible set
For one credible set, the size of the credible set is the belief that the credible set contains the causal variant (eg 90%). 

## Coverage of credible set
If we repeat analysing a list of credible sets, coverage is the amount of times the causal variant is in that credible set. Here coverage can be defined as number of times the causal variant was in the credible set out of the number of simulations run. This can be expressed as a percentage where if 10 credible sets are analysed and 9 of these credible sets include the causal variant, there is 90% coverage. \emph{Coverage is a frequentist concept because of the nature of repeatedability being used to assess Bayesian inferences}. 

## Link with Frequentist 
Link between a frequentist confidence interval and a bayesian credible interval with an uninformative prior. When Bayesian framework is used, but the prior belief is uninformative, the posterior probability is derived from the observed data, defined by the likelihood. An uninformative prior mathematically takes the form of uniform distribution, so that any probability in the distribution has the same proportion. This is similar to frequentist probabilities, which only use the data to estimate the unknown parameter. Under this framework, a parameter of interest credible interval, the posterior probability a SNP is causal can be interpreted. 

# Scope of the Study: 
This study is done under the one causal variant assumption, meaning there is only one causal variant per credible set. This assumption creates some limitations that are outlined here. 
Colocalization is when a single SNP is associated with multiple phenotypes. Some methods used for colocalization calculate posterior probabilities works by fine mapping each trait under a single causal variant assumption. The two posterior probabilites are then integrated over to calculate probabilities that those variants are shared [@Fortune2018]. Therefore, this analysis may be able to be extended to involve colocalization in the future but is not addressed at this time.  

Copy number variants (CNVs) and variable number tandem repeat (VNTRs) consisting of microsatellites and minisatellites mutations, may account for a 10-15% of heritable gene expression variation in humans [@Gymrek2016]. Repetitive DNA is not easily analysed by next generation DNA sequencing methods, which struggle with homopolymeric tracts, that is, parts of the genotype that have the same base pair type repeated many times. Bi-allelic SNPs are not subject to this issue in the way CNVs and VNTRs are, which means they are more easily assayed. This analysis does not address the potential importance of CNVs and VNTRs and assumes that the causal variant can is a bi-allelic SNP. 

Functional annotations are added information known about a SNP in how it effects expressed phenotypes. Functional annotations are typically expressed as coding or non-coding regions of the genes. Among functional annotations are expression quantitative trait loci (eQTL) where a SNP is known to effect a level of expression a particular gene in a particular tissue. Functional annotations are important because phyiscal distance to a gene is not substantive evidence of causality. This means a mutation farther from a gene may play an important role in regulation of that specific gene's expression. 
Although not explored here, methods outlined in this study could be explored by altering the assigned prior value of a SNP in finemap.abf. Reweighting of the posterior probabilites can be done by using fGWAS [Pickrell2014] or PAINTOR methods [@Kichaev2014]. 

Different trans-ancestries have different LD patterns. This study does not address any trans-ancestry differences or geographical analysis of subpopulations and their respective potential subtypes.  A SNP that has a small p-value across groups that have different genetic architecture shows a stronger genetic assocation to a phenotype than if it were present in only one of the groups. In this analysis only one LD matrix is considered at a time, but this could be extended in the future, particularly with relevant open data access from sources such as 1000 Genomes Project. [find spot to cite Jenna's papers here ]. 

## Aims 
This study aims to prove that current fine mapping practices could be made more efficient to obtain a smaller credible set size. To do this, a metric to capture the amount of disorder in the system - the set of SNPs, was created. Through this method, we are able to say that by making the SNPs with the highest posterior probabilites always incorporated in the credible set through ordering, this contains more information about each SNP. By incorporating the information that SNPs with high posterior probabilities are more likely to be causual, the specified threshold can be reached using a smaller credible set.

## Should go somewhere else soon, but in the flow 

The measure of disorder can be viewed as a statistical annotation that allows us to have more confidence that the causal variant is contained in the credible set, when candidate SNP(s) have much higher posterior probabilities than the entire full set of SNPs being tested. 

# Current Practices in Fine Mapping 
Currently it is typical practice that credible sets be ordered by descending posterior probabilities to order the SNP with the highest posterior probability first and the SNP with the lowest posterior probability last. This practice is done because it reduces the credible set, that is the number of candidate SNPs to consider. 

Under this practice, the relationship to size and coverage are considered. 

## Relationships to Ordering 
### Size
The relationship between ordering and credible set size is an inverse relationship. This is simple to see in an example. Say there are 100 SNPs and the 97th SNP is the causal variant with the lowest p-value, and therefore has the highest posterior probability. An unordered credible set with a pre-specified threshold (cumulative sum of posterior probabilities), would need to take 97 SNPs to achieve the specified threshold. The ordered credible set could achieve the same threshold with many fewer SNPs. This is illustrated below. 

[FIGURE FOR DIFFERENCES IN SIZE]. 

### Coverage 
Bayesian credible intervals (continuous) are well understood not to have their coverage affected by ordering. [Why???Need to investigate]
The same claims have been extended to credible sets (discrete). 
In this analysis, we prove these claims create overcoverage. This is due to not accounting for the amount of disorder in the system, created by the differ

### Entropy
A measure of a system's disorder. 
-sum p log(p)

# Methods

## Why use Simulations?
Simulations are the gold standard in evaluating methodologies relevant to causal variants. In the "real-world", that is genotyping for actual phenotypes of interest (typically a disease), we do not know the causal variant. It is not possible evaluate both real-world data and the robustness of the methodology simulataneously. This is because the lack of certainty and variability of both parameters may be the reason for the outcomes; a basic concept of the scientific method -- test one variable at a time and hold all other variables the same as controls. 

## Running Simulations

Simulations were run by through a series of steps, leveraging relevant available R packages. Full code described here is available on: https://github.com/HopeMWatson/credibleset_thesis/blob/master/coverage%20.Rmd

The libraries utilised in this analysis were devtools to load the simGWAS and coloc packages directly from github. ggplot2 package was used to produce figures. 

### simGWAS
The simGWAS package directly simulates GWAS summary data, without indivudal data as an intermediate step. The expected statistics are mathemathically derived for any set of causal variants and their effects sizes, conditional upon control haplotype frequencies [@Fortune2018]. The arguements for simGWAS to run require a specifation of the causal SNP, its effect size (OR), and allele (MAF) and haplotypes frequencies. simGWAS then produce simulated z-scores for each SNP. A list of z-scores was created for each SNP by running the simulation 100 times. The final output from simGWAS was a 100x100 matrix, which contained data on 100 SNPs with 100 z-scores. The z-scores were turned into p-values. 

### Finemap.abf 
From the assigned p-values, posterior probailites are calculated. This process was outlined above, showing how approximate bayes factors (ABF) are created from the observed data and the uniformative prior. The finemap.abf function works on summary statistic data, inputs as either 1) p-values, as used here or 2) coefficients and coefficient variance. Other functions in the coloc package can be utilised for full genotyped data with different statistical approaches. 

The finemap.abf function works by specifying input arguements for 1) p-values 2) sample size 3) MAF 4) ratio of cases to controls (s), and 5) type =cc. The prior is set to p=1e-04 as a default for an uniformative prior. An empty matrix was created in to map each to place each posterior probability into the list back into the dataframe. The finemap.abf function was then looped over each SNP and its respective list of p-values to create posterior probabilities. The final output from finemap.abf was a 100x100 matrix, which contained data on 100 SNPs with 100 respective posterior probabilites of the SNP being the specified causal variant. 

### Credset 


## Coverage Evauluation

how you fixed (if we do!)

# Results

## Different Scenarios 

Different size and coverage scenarios were explored to analyse the relationship ordering [PICK UP FROM HERE TOMORROW]

plots of coverage vs size for unordered 'credible sets'

plots of coverage vs size

plots of entropy vs coverage-size

fix!

# Discussion

# References

\bibliographystyle{unsrt} 
